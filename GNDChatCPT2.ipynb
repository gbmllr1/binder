{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fd39e2-53b9-4d46-8d82-bb0796da2335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import PyPDF2\n",
    "import re\n",
    "import csv\n",
    "import logging, sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "logging.basicConfig(filename='reiseberichte.log', encoding='utf-8', level=logging.DEBUG) # Fehlermeldungen etc nach datei loggen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1065f4-7007-4040-9039-c07edc1ac971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test-pdfs\n",
    "protokoll_file = (r\"Sitzung Thalmann_26.6.2023_beispiel.pdf\") #relative Pfade, bei lokalen notebooks absolute besser\n",
    "artikel_file = (r\"fables.pdf\")\n",
    "philo_file = (r\"scotus.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cf3b5f-09bf-4ea7-8517-b03669c126f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_pdf(file_path): \n",
    "    # PDF öffnen und den Text extrahieren\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8938cbf8-4c6d-4965-ab5d-d53e24ebdf05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitzung Thalmann  \n",
      "Uni Zürich, immer deutschschweiz-romandie, 15 Juni das letzte Mal. Über 20 personen. \n",
      "Konzept: Konferenz mit Apéro für Austausch. Veranstaltung ist gratis für die Mitglieder. APéro wird \n",
      "von Bibliosuisse beza hlt.  \n",
      "Stefan Wiederkehr. \n",
      "1. Marketing\n",
      "Citizen Scienc e 10 Personen, vielleicht  \n",
      "Intervie w mit Stefan Wiederkehr zu CItiz en Science \n",
      "4-5 Frag en um die Themati k klar zu machen. Vielleich t an einem Beispiel deutli ch machen.\n",
      "(Anfang August) \n",
      "Lilly fragen wegen Bibliosui sse Intervie w posten \n",
      "Anfragen w egen ZB zum posten wegen Interview \n",
      "Bibliosuisse info möglich?  \n",
      "5.7. Mail geht Raus  \n",
      "Etwas für d ie Werbung überlegen \n",
      "Nach dem Anlass ein Berich t schreib en mit Fotos. \n",
      "2. Organisatorisch\n",
      "Vorbereitung für die Tagung  \n",
      "Apéro von 18-19 Uhr  \n",
      "10-20 Personen, mit Reserv e au f 25 Personen \n",
      "20 Fr. pro Person.  \n",
      "Eine Woche im Voraus definitiv e Anzahl \n",
      "Mit Stefan Wiederkehr Kontakt aufn ehmen, was er braucht. \n"
     ]
    }
   ],
   "source": [
    "print(read_pdf(protokoll_file)) # Textextraktion testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f9a24e-06c0-43cc-a424-dbcee009ec66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_words(text):\n",
    "    # Interpunktion löschen und Wörter trennen, und ein array aller vorkommenden Wörter zurückgeben\n",
    "#    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[,.:;?!\\(\\)\\'0-9]+\\s', ' ', text) # Interpunktion und Zahlen entfernen, wenn sie von einem whitespace gefolgt werden \n",
    "    text = re.sub(r'[,.:;?!\\(\\)\\'0-9]+', '', text) # übrige Interpunktion und Zahlen entfernen\n",
    "    text = re.sub(r'\\n', '', text) # Zeilenumbrüche entfernen, falls einer mitten in einem Wort ist\n",
    "    all_words = text.split()\n",
    "    unique_words = set(all_words)\n",
    "    \n",
    "#    for word in all_words:\n",
    "#        if not word in unique_words:\n",
    "#            unique_words.append(word)\n",
    "    #words = capitalize_words(words)\n",
    "\n",
    "    return list(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32eb6928-9b25-44fd-929f-e9752791e176",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "464\n",
      "3103\n"
     ]
    }
   ],
   "source": [
    "print(len(split_words(read_pdf(protokoll_file))))\n",
    "print(len(split_words(read_pdf(artikel_file))))\n",
    "print(len(split_words(read_pdf(philo_file))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac10841-f80a-427c-80d4-128f1cc70132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sitzung', 'Thalmann', 'Uni', 'Zürich', 'immer', 'deutschschweiz-romandie', 'Juni', 'das', 'letzte', 'Mal', 'Über', 'personen', 'Konzept', 'Konferenz', 'mit', 'Apéro', 'für', 'Austausch', 'Veranstaltung', 'ist', 'gratis', 'die', 'Mitglieder', 'APéro', 'wird', 'von', 'Bibliosuisse', 'beza', 'hlt', 'Stefan', 'Wiederkehr', 'MarketingCitizen', 'Scienc', 'e', 'Personen', 'vielleicht', 'Intervie', 'w', 'zu', 'CItiz', 'en', 'Science', '-', 'Frag', 'um', 'Themati', 'k', 'klar', 'machen', 'Vielleich', 't', 'an', 'einem', 'Beispiel', 'deutli', 'ch', 'Anfang', 'August', 'Lilly', 'fragen', 'wegen', 'Bibliosui', 'sse', 'posten', 'Anfragen', 'egen', 'ZB', 'zum', 'Interview', 'info', 'möglich', 'Mail', 'geht', 'Raus', 'Etwas', 'd', 'ie', 'Werbung', 'überlegen', 'Nach', 'dem', 'Anlass', 'ein', 'Berich', 'schreib', 'Fotos', 'OrganisatorischVorbereitung', 'Tagung', 'Uhr', 'Reserv', 'au', 'f', 'Fr', 'pro', 'Person', 'Eine', 'Woche', 'im', 'Voraus', 'definitiv', 'Anzahl', 'Mit', 'Kontakt', 'aufn', 'ehmen', 'was', 'er', 'braucht']\n",
      "['Analogous', 'Unity', 'in', 'the', 'Writings', 'of', 'John', 'Duns', 'Scotus', 'Domenic', 'DEttoreJournal', 'History', 'Philosophy', 'Volume', 'Number', 'October', 'pp', '-', 'Article', 'Published', 'by', 'Johns', 'Hopkins', 'University', 'PressDOI', 'For', 'additional', 'information', 'about', 'this', 'articleFor', 'content', 'related', 'to', 'articlehttps//doiorg//hph', 'https//musejhuedu/article/', 'https//musejhuedu/related_contenttype=article&id=', '*', 'D’Ettore', 'is', 'an', 'associate', 'professor', 'philosophy', 'at', 'Marian', 'Journal', 'vol', 'no', '–']\n"
     ]
    }
   ],
   "source": [
    "protokoll_allwords = split_words(read_pdf(protokoll_file))\n",
    "philo_allwords = split_words(read_pdf(philo_file)) \n",
    "\n",
    "print(protokoll_allwords)\n",
    "print(philo_allwords[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0c6167-5449-41fd-ab1b-f7dd4d455a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def capitalize_words(word_list):\n",
    "#    capitalized_words = []\n",
    "#    for word in word_list:\n",
    "#        capitalized_words.append(word.capitalize())\n",
    "#    return capitalized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de69a5b5-8c08-49ce-a3aa-cd6336c0eed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(lst, stpwds):\n",
    "    newlst = lst[:] #In neue Variable kopieren, sodass die alte erhalten bleibt\n",
    "    for element in stpwds:\n",
    "        for word in newlst:\n",
    "            if word == element:\n",
    "                newlst.remove(word)\n",
    "    return newlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eff43a2-066f-4827-a36d-77777ea8a066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    stopwords = [\"und\", \"oder\", \"der\", \"die\", \"das\", \"von\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "921024d6-9071-4e03-966b-5be65cfe601c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PDF aufrufen, durchsuchbar machen und bereinigen.\n",
    "words = remove_stopwords(philo_allwords, stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab564b44-5de4-4f38-8023-1f54d4dea5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3103\n",
      "3102\n"
     ]
    }
   ],
   "source": [
    "print(len(philo_allwords))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0c62ae-6858-4648-b628-5da8df1d36f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test ob Wörter gelöscht wurden.##keine anwendung?\n",
    "def find_element(lst, element):\n",
    "    if element in lst:\n",
    "        index = lst.index(element)\n",
    "        return index\n",
    "    else:\n",
    "        return -1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4326d2e2-da41-4e3f-b70b-8c2c5cdfad4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_and_write(word_list, output_file):\n",
    "   \n",
    "        word_column = []\n",
    "        previous_words_column = []\n",
    "        next_words_column = []\n",
    "\n",
    "        for word in word_list:\n",
    "            if search_gnd(word):\n",
    "                word_column.append(word)\n",
    "                if word_list.index(word) == 0:\n",
    "                    string = \"No previous words.\"\n",
    "                elif word_list.index(word) == 1:\n",
    "                    string = \"No word here\" + \" \" + word_list[word_list.index(word) - 2] + \" \" + word_list[word_list.index(word) - 1]\n",
    "                elif word_list.index(word) == 2:\n",
    "                    string = \"No word here\" + \" \" + \"No word here\" + \" \" + word_list[word_list.index(word) - 1]\n",
    "                else:\n",
    "                    string = (word_list[word_list.index(word) - 3]) + \" \" + word_list[word_list.index(word) - 2] + \" \" + word_list[word_list.index(word) - 1]\n",
    "                previous_words_column.append(string)  \n",
    "            \n",
    "                \n",
    "                if word_list.index(word) == len(word_list) -1:\n",
    "                    string = \"No next words.\"\n",
    "                elif word_list.index(word) == len(word_list) - 2:\n",
    "                    string = word_list[word_list.index(word) + 1] + \" \" + \"No word here\" + \" \" + \"No word here\"\n",
    "                elif word_list.index(word) == len(word_list) - 3:\n",
    "                    string = word_list[word_list.index(word) + 1] + \" \" + word_list[word_list.index(word) + 2] + \" \" + \"No word here\"\n",
    "                else:\n",
    "                    string = (word_list[word_list.index(word) + 1]) + \" \" + word_list[word_list.index(word) + 2] + \" \" + word_list[word_list.index(word) + 3]\n",
    "                next_words_column.append(string) \n",
    "\n",
    "        with open(output_file, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "\n",
    "            # Schreibe den Header\n",
    "            writer.writerow(['Word', 'Previous Words', 'Next Words'])\n",
    "\n",
    "            # Schreibe die Daten aus den Listen\n",
    "            for i in range(len(word_column)):\n",
    "                writer.writerow([word_column[i], previous_words_column[i], next_words_column[i]])\n",
    "\n",
    "        print(f'Die Datei {output_file} wurde erfolgreich erstellt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca49e9e9-05ca-41e1-83d3-fbafbb131c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output_file = 'output.csv'\n",
    "#search_and_write(fewer_words, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbb100f-2553-4a87-afe2-e385b0a2ae5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_word_in_geonames(word):\n",
    "    url = f\"http://api.geonames.org/searchJSON?name={word}&username=gbmllr\" #&fuzzy=0.8 auch möglich\n",
    "\n",
    "    response = requests.get(url)\n",
    "        \n",
    "    if response.status_code == 200:\n",
    "        geonames_json = response.json()['geonames']\n",
    "        if not geonames_json == []: # if geonames_json:\n",
    "            if geonames_json[0]:\n",
    "                geonames = geonames_json[0]\n",
    "            else:\n",
    "                geonames = geonames_json\n",
    "            if 'countryname' in geonames:   \n",
    "                print('\"' + word + '\"' + ' ist ein Ort in' + geonames[0]['countryname'])\n",
    "            else:\n",
    "                print('\"' + word + '\"' + ' ist ein Ort')\n",
    "            return True\n",
    "        else:\n",
    "            print('Keine Treffer für', word)\n",
    "            return False\n",
    "    else:\n",
    "        print('HTML-Status für %s war', word, response.status_code)\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ef6b5b5-0db3-43c4-84e9-c9818fc00ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Spain\" ist ein Ort\n",
      "Keine Treffer für Summaries\n",
      "Keine Treffer für Logic\n",
      "Keine Treffer für “Omne\n",
      "Keine Treffer für expediens\n"
     ]
    }
   ],
   "source": [
    "for word in words[2050:2055]:\n",
    "    check_word_in_geonames(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf363461-2303-4247-b87b-ce715fe3709a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geonames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mgeonames\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'geonames' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9faebac-f452-4a06-87e0-d7119e6e8b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_gnd(word):\n",
    "    # GND-API-URL für die Suche nach Geographika\n",
    "    url = \"https://lobid.org/gnd/search?q={}&filter=%2B%28type%3APlaceOrGeographicName%29\".format(word)\n",
    "\n",
    "    # Führe die Anfrage an die GND-API aus\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Überprüfe den Statuscode der Antwort\n",
    "    if response.status_code == 200:\n",
    "        # Extrahiere die JSON-Daten aus der Antwort\n",
    "        data = response.json()\n",
    "\n",
    "        # Überprüfe, ob es Ergebnisse gibt\n",
    "        if \"member\" in data and len(data[\"member\"]) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        # Bei einem Fehlercode wird False zurückgegeben\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719dba9f-5fc6-4b34-b021-881094eb3853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def has_wikidata_population(word):\n",
    "    # Erstelle eine SPARQLWrapper-Instanz für die Wikidata-Abfrage-URL\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "\n",
    "    # Setze das SPARQL-Abfrage-Skript mit dem übergebenen Wort\n",
    "    query = f\"\"\"\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "    SELECT ?entity ?population\n",
    "    WHERE {{\n",
    "      ?entity wdt:P1082 ?population ;\n",
    "              rdfs:label \"{word}\"@en .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Setze das Abfrage-Skript in die SPARQLWrapper-Instanz\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    # Setze das Ausgabeformat auf JSON\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    # Führe die SPARQL-Abfrage aus und erhalte die Ergebnisse\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    # Überprüfe, ob es Ergebnisse gibt\n",
    "    if len(results[\"results\"][\"bindings\"]) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba30c8-b13a-4854-90fc-4a09337aaedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
